{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "clq5fcVdFOpu",
   "metadata": {
    "id": "clq5fcVdFOpu"
   },
   "source": [
    "Colab Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af6757da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1743512747509,
     "user": {
      "displayName": "Giovanni CalabrÃ²",
      "userId": "13146937522042262894"
     },
     "user_tz": -120
    },
    "id": "af6757da",
    "outputId": "9252c26c-a52b-4bb8-b9b5-8e403d228aff"
   },
   "outputs": [],
   "source": [
    "### install pyro-ppl for Deep learning model (BNN) --- if using Colab\n",
    "# !pip install pyro-ppl\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd83fb",
   "metadata": {
    "id": "b5cd83fb"
   },
   "source": [
    "### Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61ffcd9",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1743511976502,
     "user": {
      "displayName": "Giovanni CalabrÃ²",
      "userId": "13146937522042262894"
     },
     "user_tz": -120
    },
    "id": "a61ffcd9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "# import pyro\n",
    "# import pyro.distributions as dist\n",
    "# from pyro.nn import PyroModule, PyroSample\n",
    "# from pyro.infer import SVI, Trace_ELBO\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.special import gammaln\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor' if torch.cuda.is_available() else 'torch.FloatTensor')\n",
    "###pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2e123",
   "metadata": {},
   "source": [
    "### Set Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "518c59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'orlÃ©ans'\n",
    "scenario_name = 'DRT'\n",
    "directory = f\"study areas/{city}/\"\n",
    "\n",
    "# OSRM server URLs\n",
    "# The first url is for computing walking distance, the second one for car driving distance\n",
    "osrm_url_walk = \"http://localhost:5000\"  # Change if your OSRM server is at a different URL\n",
    "osrm_url_car = \"http://localhost:5001\"   # Change if your OSRM server is at a different URL\n",
    "\n",
    "# Study area boundaries (EPSG:4326)\n",
    "bbox = [1.7676734, 47.7601594, 2.1089651, 48.0134634]  # orleans metropole area\n",
    "\n",
    "# File paths\n",
    "trips_input_filename = str(directory + 'trips_DRT_processed.csv')  # File is in the same folder as script\n",
    "\n",
    "# path to the csv of generated virtual DRT trips trips\n",
    "generated_virtual_drt_trips = str(directory + 'all_synthetic_trips.csv')\n",
    "# path to the csv of generated trips (with ML predicted travel times) # Optional output path\n",
    "trips_output_path_generated_drt_trips = f\"{directory}virtual_trips_with_predicted_thresholds.CSV\"\n",
    "#path to PT gtfs files\n",
    "gtfs_path = \"study areas/orlÃ©ans/gtfs/gtfs_PT.zip\"\n",
    "\n",
    "# # input features and target - file \"trips_{scenario_name}.csv\"\n",
    "input_features = ['origin_lat', 'origin_lon', 'destination_lat', 'destination_lon',\n",
    "        'departure_time_hour', 'departure_time_minute',\n",
    "       'departure_time_day_of_week', 'departure_time_day_of_month',\n",
    "       'departure_time_month', 'departure_time_hour_sin',\n",
    "       'departure_time_hour_cos', 'departure_time_day_of_week_sin',\n",
    "       'departure_time_day_of_week_cos', 'departure_time_month_sin',\n",
    "       'departure_time_month_cos', 'distance'] \n",
    "\n",
    "target_features = ['travel_time'] #, 'waiting_time'\n",
    "                \n",
    "\n",
    "# set departure times the same as the ones used in trips generation (previous notebook)\n",
    "day = '2025-04-07'\n",
    "day_gtfs = '20250407'\n",
    "hours = [8, 12, 16, 20]\n",
    "departure_times = [f\"{day} {hour:02d}:00:00\" for hour in hours]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ''' only when running in colab\n",
    "# import locale\n",
    "# # Set the locale to UTF-8\n",
    "# locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "# # Define the fixed part of the path\n",
    "# base_path = '/content/drive/MyDrive/ColabNotebooks/AccessibilityDRTcityChrone_CL/'\n",
    "# # Now, run your command:\n",
    "# !ls '/content/drive/MyDrive/ColabNotebooks/AccessibilityDRTcityChrone_CL/trips/Caltanissetta'\n",
    "# legs_input_path = base_path + 'trips/trips_DRT_pr.CSV'\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71bf951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš— Bayesian Travel Time Prediction Library - Comprehensive Example\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š STEP 1: Read Data\n",
      "\n",
      "Dataset Summary:\n",
      "  Input features: ['origin_lat', 'origin_lon', 'destination_lat', 'destination_lon', 'departure_time_hour', 'departure_time_minute', 'departure_time_day_of_week', 'departure_time_day_of_month', 'departure_time_month', 'departure_time_hour_sin', 'departure_time_hour_cos', 'departure_time_day_of_week_sin', 'departure_time_day_of_week_cos', 'departure_time_month_sin', 'departure_time_month_cos', 'distance']\n",
      "  Target feature: travel_time\n",
      "  Training samples: 12\n",
      "  Test samples: 4\n",
      "\n",
      "ðŸ¤– STEP 2: Model Comparison\n",
      "============================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "============================================================\n",
      "Dataset: 12 train, 4 test samples\n",
      "Features: ['origin_lat', 'origin_lon', 'destination_lat', 'destination_lon', 'departure_time_hour', 'departure_time_minute', 'departure_time_day_of_week', 'departure_time_day_of_month', 'departure_time_month', 'departure_time_hour_sin', 'departure_time_hour_cos', 'departure_time_day_of_week_sin', 'departure_time_day_of_week_cos', 'departure_time_month_sin', 'departure_time_month_cos', 'distance']\n",
      "Target: ['travel_time']\n",
      "Distributions: {'linear': 'gamma', 'rf': 'gamma', 'bnn': 'gamma'}\n",
      "BNN Config: Prior=normal, Likelihood=gamma\n",
      "\n",
      "----------------------------------------\n",
      "Processing Linear Regression...\n",
      "  Distribution: gamma\n",
      "\n",
      "Training Linear Regression...\n",
      "  Training samples: 12\n",
      "  Test samples: 4\n",
      "  Features: 16\n",
      "  âœ“ Training completed successfully\n",
      "\n",
      "Linear Regression Performance:\n",
      "  MAE:  189.338\n",
      "  RMSE: 266.546\n",
      "  NLL:  6.9247\n",
      "  âœ“ Linear Regression completed successfully\n",
      "\n",
      "----------------------------------------\n",
      "Processing Random Forest...\n",
      "  Distribution: gamma\n",
      "\n",
      "Training Random Forest...\n",
      "  Training samples: 12\n",
      "  Test samples: 4\n",
      "  Features: 16\n",
      "  âœ“ Training completed successfully\n",
      "\n",
      "Random Forest Performance:\n",
      "  MAE:  371.953\n",
      "  RMSE: 408.855\n",
      "  NLL:  12.2620\n",
      "  âœ“ Random Forest completed successfully\n",
      "\n",
      "----------------------------------------\n",
      "Processing Bayesian Neural Network...\n",
      "  Prior: normal\n",
      "  Likelihood: gamma\n",
      "  Hidden units: 32\n",
      "\n",
      "Training Bayesian Neural Network...\n",
      "  Training samples: 12\n",
      "  Test samples: 4\n",
      "  Features: 16\n",
      "Training BNN with 12 samples on cuda...\n",
      "Epoch 0, Loss: 402.1247\n",
      "Epoch 20, Loss: 351.1408\n",
      "Epoch 40, Loss: 321.0924\n",
      "Epoch 60, Loss: 295.3077\n",
      "Early stopping at epoch 62\n",
      "Training completed. Best loss: 268.8458\n",
      "  âœ“ Training completed successfully\n",
      "\n",
      "Bayesian Neural Network Performance:\n",
      "  MAE:  655.701\n",
      "  RMSE: 768.928\n",
      "  NLL:  189.3985\n",
      "  âœ“ Bayesian Neural Network completed successfully\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Model                     MAE      RMSE     NLL        Distribution\n",
      "-----------------------------------------------------------------\n",
      "Linear Regression         189.338  266.546  6.9247     gamma\n",
      "Random Forest             371.953  408.855  12.2620    gamma\n",
      "Bayesian Neural Network   655.701  768.928  189.3985   gamma\n",
      "\n",
      "ðŸ† Best Model: Linear Regression (NLL: 6.9247)\n",
      "============================================================\n",
      "\n",
      "âœ… Example completed successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import scipy.special\n",
    "from typing import List, Dict, Union, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Global Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SUPPORTED_DISTRIBUTIONS = ['normal', 'lognormal', 'gamma']\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, \n",
    "                input_features: List[str], \n",
    "                target_features: List[str]) -> Tuple[np.ndarray, Union[np.ndarray, Dict]]:\n",
    "    \"\"\"\n",
    "    Prepare features and target variables for model training.\n",
    "    \n",
    "    Handles missing values by replacing NaN with 0 and ensures all target\n",
    "    values are positive (minimum 1e-6) for compatibility with positive\n",
    "    distributions like Gamma and Log-Normal.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataset containing all features and targets\n",
    "        input_features: List of column names to use as input features\n",
    "        target_features: List of column names to use as target variables\n",
    "        \n",
    "    Returns:\n",
    "        X: Input features as numpy array with NaN values replaced by 0\n",
    "        y: Target values - numpy array if single target, dict if multiple targets.\n",
    "           All values are guaranteed to be positive (>= 1e-6)\n",
    "           \n",
    "    Example:\n",
    "        >>> df = pd.DataFrame({\n",
    "        ...     'distance': [10, 20, np.nan],\n",
    "        ...     'travel_time': [15, 30, 25]\n",
    "        ... })\n",
    "        >>> X, y = prepare_data(df, ['distance'], ['travel_time'])\n",
    "        >>> print(X)  # [[10], [20], [0]]  # NaN replaced with 0\n",
    "        >>> print(y)  # [15, 30, 25]\n",
    "    \"\"\"\n",
    "    # Prepare input features - handle missing values\n",
    "    X = df[input_features].values\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Prepare target variables - ensure positive values\n",
    "    if len(target_features) == 1:\n",
    "        # Single target - return as array\n",
    "        y = np.maximum(df[target_features[0]].values, 1e-6)\n",
    "    else:\n",
    "        # Multiple targets - return as dictionary\n",
    "        y = {target: np.maximum(df[target].values, 1e-6) \n",
    "             for target in target_features}\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def evaluate_predictions(y_true: np.ndarray, \n",
    "                        predictions: Dict[str, np.ndarray], \n",
    "                        distribution: str = 'gamma') -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics for probabilistic predictions.\n",
    "    \n",
    "    Computes Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and\n",
    "    distribution-specific Negative Log-Likelihood (NLL) for model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True target values\n",
    "        predictions: Dictionary containing 'mean' and 'std' predictions\n",
    "        distribution: Distribution type for NLL calculation\n",
    "                     ('normal', 'lognormal', or 'gamma')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics:\n",
    "        - 'mae': Mean Absolute Error\n",
    "        - 'rmse': Root Mean Square Error  \n",
    "        - 'nll': Negative Log-Likelihood (distribution-specific)\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If unsupported distribution is specified\n",
    "        \n",
    "    Example:\n",
    "        >>> y_true = np.array([2.0, 3.0, 4.0])\n",
    "        >>> predictions = {\n",
    "        ...     'mean': np.array([2.1, 2.9, 4.1]),\n",
    "        ...     'std': np.array([0.2, 0.2, 0.2])\n",
    "        ... }\n",
    "        >>> metrics = evaluate_predictions(y_true, predictions, 'gamma')\n",
    "        >>> print(f\"MAE: {metrics['mae']:.3f}\")\n",
    "    \"\"\"\n",
    "    y_pred = predictions['mean']\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Basic point prediction metrics\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "    # Distribution-specific Negative Log-Likelihood calculation\n",
    "    predicted_std = predictions['std'] + epsilon\n",
    "    \n",
    "    if distribution == 'normal':\n",
    "        # Normal distribution NLL: -log(N(y|Î¼,ÏƒÂ²))\n",
    "        nll = (0.5 * np.mean((y_true - y_pred)**2 / predicted_std**2) + \n",
    "               0.5 * np.mean(np.log(2 * np.pi * predicted_std**2)))\n",
    "               \n",
    "    elif distribution == 'lognormal':\n",
    "        # Log-Normal distribution NLL: -log(LogN(y|Î¼,ÏƒÂ²))\n",
    "        y_true = np.maximum(y_true, epsilon)\n",
    "        y_pred = np.maximum(y_pred, epsilon)\n",
    "        \n",
    "        log_mean = np.log(y_pred)\n",
    "        log_std = np.maximum(predicted_std / y_pred, epsilon)\n",
    "        \n",
    "        nll = (0.5 * np.mean((np.log(y_true) - log_mean)**2 / log_std**2) + \n",
    "               0.5 * np.mean(np.log(2 * np.pi * log_std**2)) + \n",
    "               np.mean(np.log(y_true)))\n",
    "               \n",
    "    elif distribution == 'gamma':\n",
    "        # Gamma distribution NLL: -log(Gamma(y|Î±,Î²))\n",
    "        alpha = np.maximum((y_pred / predicted_std)**2, epsilon)\n",
    "        beta = np.maximum(y_pred / (predicted_std**2), epsilon)\n",
    "        y_true = np.maximum(y_true, epsilon)\n",
    "        \n",
    "        gamma_loglik = (alpha * np.log(beta) - \n",
    "                       scipy.special.gammaln(alpha) + \n",
    "                       (alpha - 1) * np.log(y_true) - \n",
    "                       beta * y_true)\n",
    "        nll = -np.mean(gamma_loglik)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. \"\n",
    "                        f\"Choose from: {SUPPORTED_DISTRIBUTIONS}\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'nll': nll}\n",
    "\n",
    "\n",
    "class BNNTravelTimeModel(PyroModule):\n",
    "    \"\"\"\n",
    "    Bayesian Neural Network core model with configurable distributions.\n",
    "    \n",
    "    A single hidden layer neural network with Bayesian weights and biases.\n",
    "    Supports different prior distributions for weights and different likelihood\n",
    "    distributions for outputs.\n",
    "    \n",
    "    Architecture:\n",
    "        Input -> Linear(input_dim, hidden_dim) -> ReLU -> Linear(hidden_dim, 2) -> Output\n",
    "        \n",
    "    The output layer produces 2 values which are interpreted based on the\n",
    "    likelihood distribution:\n",
    "    - Normal: mean and log(std)\n",
    "    - Log-Normal: log(mean) and log(std)\n",
    "    - Gamma: raw mean and log(variance) parameters\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        hidden_dim: Size of hidden layer (default: 16)\n",
    "        prior_dist: Prior distribution for weights ('normal', 'lognormal', 'gamma')\n",
    "        likelihood_dist: Output likelihood distribution ('normal', 'lognormal', 'gamma')\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If unsupported distribution types are specified\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 16, \n",
    "                 prior_dist: str = 'normal', \n",
    "                 likelihood_dist: str = 'gamma'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validate distribution parameters\n",
    "        if prior_dist not in SUPPORTED_DISTRIBUTIONS:\n",
    "            raise ValueError(f\"Unsupported prior: {prior_dist}\")\n",
    "        if likelihood_dist not in SUPPORTED_DISTRIBUTIONS:\n",
    "            raise ValueError(f\"Unsupported likelihood: {likelihood_dist}\")\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prior_dist = prior_dist\n",
    "        self.likelihood_dist = likelihood_dist\n",
    "\n",
    "        # Create prior distribution for weights and biases\n",
    "        prior_dist_obj = self._create_prior_distribution(prior_dist)\n",
    "\n",
    "        # Define Bayesian network layers\n",
    "        # Hidden layer with Bayesian weights\n",
    "        self.fc1 = PyroModule[nn.Linear](input_dim, hidden_dim)\n",
    "        self.fc1.weight = PyroSample(\n",
    "            prior_dist_obj.expand([hidden_dim, input_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc1.bias = PyroSample(\n",
    "            prior_dist_obj.expand([hidden_dim]).to_event(1)\n",
    "        )\n",
    "\n",
    "        # Output layer with Bayesian weights (2 outputs for distribution parameters)\n",
    "        self.fc2 = PyroModule[nn.Linear](hidden_dim, 2)\n",
    "        self.fc2.weight = PyroSample(\n",
    "            prior_dist_obj.expand([2, hidden_dim]).to_event(2)\n",
    "        )\n",
    "        self.fc2.bias = PyroSample(\n",
    "            prior_dist_obj.expand([2]).to_event(1)\n",
    "        )\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def _create_prior_distribution(self, prior_dist: str):\n",
    "        \"\"\"\n",
    "        Create prior distribution object for network weights.\n",
    "        \n",
    "        Args:\n",
    "            prior_dist: Type of prior distribution\n",
    "            \n",
    "        Returns:\n",
    "            Pyro distribution object configured for the specified type\n",
    "        \"\"\"\n",
    "        if prior_dist == 'normal':\n",
    "            # Standard normal prior: N(0, 0.1Â²)\n",
    "            loc = torch.tensor(0.0, dtype=torch.float32, device=DEVICE)\n",
    "            scale = torch.tensor(0.1, dtype=torch.float32, device=DEVICE)\n",
    "            return dist.Normal(loc, scale)\n",
    "        elif prior_dist == 'lognormal':\n",
    "            # Log-normal prior: LogN(0, 0.1Â²)\n",
    "            loc = torch.tensor(0.0, dtype=torch.float32, device=DEVICE)\n",
    "            scale = torch.tensor(0.1, dtype=torch.float32, device=DEVICE)\n",
    "            return dist.LogNormal(loc, scale)\n",
    "        elif prior_dist == 'gamma':\n",
    "            # Gamma prior: Gamma(1.1, 10.0)\n",
    "            concentration = torch.tensor(1.1, dtype=torch.float32, device=DEVICE)\n",
    "            rate = torch.tensor(10.0, dtype=torch.float32, device=DEVICE)\n",
    "            return dist.Gamma(concentration, rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Bayesian network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input features tensor\n",
    "            y: Target values tensor (for training, None for prediction)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted mean values\n",
    "        \"\"\"\n",
    "        # Ensure tensors are on correct device\n",
    "        x = x.to(DEVICE).float()\n",
    "        if y is not None:\n",
    "            y = y.to(DEVICE).float()\n",
    "\n",
    "        # Forward pass through network\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        network_output = self.fc2(hidden)\n",
    "\n",
    "        return self._apply_likelihood(network_output, x, y)\n",
    "\n",
    "    def _apply_likelihood(self, network_output: torch.Tensor, \n",
    "                         x: torch.Tensor, y: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply likelihood distribution to network outputs.\n",
    "        \n",
    "        Args:\n",
    "            network_output: Raw network outputs (batch_size, 2)\n",
    "            x: Input features (for batch size)\n",
    "            y: Target values (None for prediction)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted mean values\n",
    "        \"\"\"\n",
    "        if self.likelihood_dist == 'normal':\n",
    "            # Normal likelihood: N(Î¼, ÏƒÂ²)\n",
    "            mean = network_output[..., 0]\n",
    "            std = torch.clamp(self.softplus(network_output[..., 1]) + 1e-6, \n",
    "                            min=1e-6, max=10.0)\n",
    "            \n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                pyro.sample(\"obs\", dist.Normal(mean, std), obs=y)\n",
    "            return mean\n",
    "                \n",
    "        elif self.likelihood_dist == 'lognormal':\n",
    "            # Log-Normal likelihood: LogN(Î¼, ÏƒÂ²)\n",
    "            log_mean = network_output[..., 0]\n",
    "            log_std = torch.clamp(self.softplus(network_output[..., 1]) + 1e-6, \n",
    "                                min=1e-6, max=5.0)\n",
    "            \n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                pyro.sample(\"obs\", dist.LogNormal(log_mean, log_std), obs=y)\n",
    "            return torch.exp(log_mean)\n",
    "                \n",
    "        else:  # gamma\n",
    "            # Gamma likelihood: Gamma(Î±, Î²)\n",
    "            mean_raw = network_output[..., 0]\n",
    "            log_var = self.softplus(network_output[..., 1]) + 1e-6\n",
    "            \n",
    "            # Convert to mean and variance, then to shape and rate\n",
    "            mean = torch.exp(mean_raw) + 1e-6\n",
    "            variance = torch.exp(log_var) + 1e-6\n",
    "            \n",
    "            rate = torch.clamp(mean / variance, min=1e-3, max=100.0)\n",
    "            shape = torch.clamp(mean * rate, min=1e-3, max=100.0)\n",
    "\n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                pyro.sample(\"obs\", dist.Gamma(shape, rate), obs=y)\n",
    "            return mean\n",
    "\n",
    "\n",
    "class BNNPredictor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Bayesian Neural Network predictor with full uncertainty quantification.\n",
    "    \n",
    "    A scikit-learn compatible estimator that provides probabilistic predictions\n",
    "    with uncertainty estimates. Uses variational inference for training and\n",
    "    supports GPU acceleration.\n",
    "    \n",
    "    Features:\n",
    "    - Automatic feature scaling\n",
    "    - Early stopping to prevent overfitting  \n",
    "    - GPU acceleration when available\n",
    "    - Comprehensive uncertainty quantification\n",
    "    - Multiple prior and likelihood distributions\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features (must match len(input_features))\n",
    "        input_features: List of input feature column names\n",
    "        target_features: List of target feature column names  \n",
    "        hidden_dim: Size of hidden layer (default: 16)\n",
    "        prior_dist: Prior distribution for weights (default: 'normal')\n",
    "        likelihood_dist: Likelihood distribution for outputs (default: 'gamma')\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If input_dim doesn't match number of input_features\n",
    "        \n",
    "    Example:\n",
    "        >>> bnn = BNNPredictor(\n",
    "        ...     input_dim=3,\n",
    "        ...     input_features=['distance', 'time', 'traffic'],\n",
    "        ...     target_features=['travel_time'],\n",
    "        ...     hidden_dim=32\n",
    "        ... )\n",
    "        >>> bnn.fit(X_train, y_train)\n",
    "        >>> predictions = bnn.predict_distribution(X_test)\n",
    "        >>> print(f\"Mean: {predictions['mean']}\")\n",
    "        >>> print(f\"Uncertainty: {predictions['std']}\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, input_features: List[str], \n",
    "                 target_features: List[str], hidden_dim: int = 16, \n",
    "                 prior_dist: str = 'normal', likelihood_dist: str = 'gamma'):\n",
    "        \n",
    "        # Validate input dimensions\n",
    "        if input_dim != len(input_features):\n",
    "            raise ValueError(f\"input_dim ({input_dim}) must match number of \"\n",
    "                           f\"features ({len(input_features)})\")\n",
    "\n",
    "        # Store configuration\n",
    "        self.input_dim = input_dim\n",
    "        self.feature_names = input_features\n",
    "        self.target_features = target_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prior_dist = prior_dist\n",
    "        self.likelihood_dist = likelihood_dist\n",
    "\n",
    "        # Initialize Pyro components\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        self.model = BNNTravelTimeModel(input_dim, hidden_dim, prior_dist, likelihood_dist)\n",
    "        self.model.to(DEVICE)\n",
    "        \n",
    "        # Automatic variational guide\n",
    "        self.guide = pyro.infer.autoguide.AutoNormal(self.model)\n",
    "        self.guide.to(DEVICE)\n",
    "        \n",
    "        # Training components (initialized during fit)\n",
    "        self.svi = None\n",
    "        self.feature_means = None\n",
    "        self.feature_stds = None\n",
    "\n",
    "    def _prepare_data(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "                     y: Optional[Union[np.ndarray, dict]] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Prepare and scale data for training or prediction.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features (numpy array or pandas DataFrame)\n",
    "            y: Target values (for training only)\n",
    "            \n",
    "        Returns:\n",
    "            Scaled feature tensor, optionally with target tensor for training\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If model hasn't been fitted when making predictions\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if needed\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.feature_names)\n",
    "\n",
    "        # Extract and clean features\n",
    "        features = X[self.feature_names].values\n",
    "        features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # Initialize feature scaling during training\n",
    "        if y is not None and self.feature_means is None:\n",
    "            self.feature_means = np.mean(features, axis=0)\n",
    "            self.feature_stds = np.std(features, axis=0)\n",
    "            # Prevent division by zero for constant features\n",
    "            self.feature_stds[self.feature_stds == 0] = 1.0\n",
    "\n",
    "        # Check if model has been fitted\n",
    "        if self.feature_means is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "\n",
    "        # Apply feature scaling\n",
    "        features = (features - self.feature_means) / self.feature_stds\n",
    "        features_tensor = torch.tensor(features, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        # Prepare targets for training\n",
    "        if y is not None:\n",
    "            if isinstance(y, dict):\n",
    "                target_values = np.maximum(y[self.target_features[0]], 1e-6)\n",
    "            else:\n",
    "                target_values = np.maximum(y, 1e-6)\n",
    "            \n",
    "            # Log-transform for log-normal likelihood\n",
    "            if self.likelihood_dist == 'lognormal':\n",
    "                target_values = np.log(target_values)\n",
    "            \n",
    "            y_tensor = torch.tensor(target_values, dtype=torch.float32).to(DEVICE)\n",
    "            return features_tensor, y_tensor\n",
    "\n",
    "        return features_tensor\n",
    "\n",
    "    def fit(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "            y: Union[np.ndarray, dict]) -> 'BNNPredictor':\n",
    "        \"\"\"\n",
    "        Train the Bayesian Neural Network using variational inference.\n",
    "        \n",
    "        Uses Stochastic Variational Inference (SVI) with Adam optimizer and\n",
    "        early stopping to prevent overfitting. Training includes automatic\n",
    "        feature scaling and batch processing for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features (n_samples, n_features)\n",
    "            y: Target values (n_samples,) or dict for multiple targets\n",
    "            \n",
    "        Returns:\n",
    "            self: Fitted estimator\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If NaN values are found in prepared data\n",
    "            \n",
    "        Example:\n",
    "            >>> bnn = BNNPredictor(input_dim=2, input_features=['x1', 'x2'], \n",
    "            ...                   target_features=['y'])\n",
    "            >>> bnn.fit(X_train, y_train)\n",
    "            >>> # Model is now ready for predictions\n",
    "        \"\"\"\n",
    "        # Prepare and validate data\n",
    "        features, target = self._prepare_data(X, y)\n",
    "\n",
    "        if torch.isnan(features).any() or torch.isnan(target).any():\n",
    "            raise ValueError(\"NaN values found in prepared data\")\n",
    "\n",
    "        # Initialize training components\n",
    "        pyro.clear_param_store()\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.0001})  # Conservative learning rate\n",
    "        self.svi = SVI(self.model, self.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "        # Training hyperparameters\n",
    "        max_epochs = 200\n",
    "        patience = 15\n",
    "        batch_size = min(256, len(features) // 8)\n",
    "        \n",
    "        # Training state\n",
    "        best_loss = float('inf')\n",
    "        no_improvement = 0\n",
    "\n",
    "        print(f\"Training BNN with {len(features)} samples on {DEVICE}...\")\n",
    "\n",
    "        # Training loop with early stopping\n",
    "        for epoch in range(max_epochs):\n",
    "            epoch_losses = []\n",
    "            n_batches = len(features) // batch_size\n",
    "            indices = torch.randperm(len(features))\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(features))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_X = features[batch_indices]\n",
    "                batch_y = target[batch_indices]\n",
    "                \n",
    "                # Skip batches with NaN values\n",
    "                if torch.isnan(batch_X).any() or torch.isnan(batch_y).any():\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    loss = self.svi.step(batch_X, batch_y)\n",
    "                    if np.isfinite(loss) and loss > 0:\n",
    "                        epoch_losses.append(loss)\n",
    "                except Exception as e:\n",
    "                    print(f\"Batch error at epoch {epoch}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # Check for improvement and early stopping\n",
    "            if epoch_losses:\n",
    "                avg_loss = np.mean(epoch_losses)\n",
    "                \n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    no_improvement = 0\n",
    "                else:\n",
    "                    no_improvement += 1\n",
    "                \n",
    "                # Progress reporting\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if no_improvement >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                if no_improvement >= 5:\n",
    "                    print(\"No valid losses, stopping training\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Training completed. Best loss: {best_loss:.4f}\")\n",
    "        return self\n",
    "\n",
    "    def predict_distribution(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "                           n_samples: int = 1000) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate probabilistic predictions with full uncertainty quantification.\n",
    "        \n",
    "        Uses the trained posterior distribution to generate samples and compute\n",
    "        prediction statistics including mean, standard deviation, and full\n",
    "        predictive distribution.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features for prediction\n",
    "            n_samples: Number of posterior samples for uncertainty estimation\n",
    "                      (default: 1000)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - 'predictions': Full sample array (n_test, n_samples)\n",
    "            - 'mean': Point predictions (n_test,)\n",
    "            - 'std': Prediction uncertainties (n_test,)\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If prediction fails (model not fitted, etc.)\n",
    "            \n",
    "        \"\"\"\n",
    "        features = self._prepare_data(X)\n",
    "\n",
    "        try:\n",
    "            # Generate posterior samples\n",
    "            predictive = pyro.infer.Predictive(self.model, guide=self.guide, num_samples=n_samples)\n",
    "            with torch.no_grad():\n",
    "                predictions = predictive(features)\n",
    "            \n",
    "            # Extract samples and compute statistics\n",
    "            samples = predictions[\"obs\"].cpu().detach().numpy()\n",
    "\n",
    "            return {\n",
    "                'predictions': samples,\n",
    "                'mean': np.mean(samples, axis=0),\n",
    "                'std': np.std(samples, axis=0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make point predictions (mean of predictive distribution).\n",
    "        \n",
    "        Args:\n",
    "            X: Input features for prediction\n",
    "            \n",
    "        Returns:\n",
    "            Point predictions as numpy array\n",
    "        \"\"\"\n",
    "        return self.predict_distribution(X)['mean']\n",
    "\n",
    "    def evaluate(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "                y: Union[np.ndarray, dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate model performance with comprehensive metrics.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: True target values\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing 'metrics' and 'predictions'\n",
    "        \"\"\"\n",
    "        predictions = self.predict_distribution(X)\n",
    "        \n",
    "        # Extract target values\n",
    "        if isinstance(y, dict):\n",
    "            y_values = y[self.target_features[0]]\n",
    "        else:\n",
    "            y_values = y\n",
    "\n",
    "        metrics = evaluate_predictions(y_values, predictions, self.likelihood_dist)\n",
    "        return {'metrics': metrics, 'predictions': predictions}\n",
    "\n",
    "\n",
    "class BaseModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Base class for models with distribution support and standardized evaluation.\n",
    "    \n",
    "    Provides common interface and evaluation methods for all model types.\n",
    "    All derived models must implement predict_distribution method.\n",
    "    \n",
    "    Args:\n",
    "        distribution: Output distribution type (default: 'gamma')\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If unsupported distribution is specified\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, distribution: str = 'gamma'):\n",
    "        if distribution not in SUPPORTED_DISTRIBUTIONS:\n",
    "            raise ValueError(f\"Unsupported distribution: {distribution}. \"\n",
    "                           f\"Choose from: {SUPPORTED_DISTRIBUTIONS}\")\n",
    "        self.distribution = distribution\n",
    "    \n",
    "    def evaluate(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "                y: Union[np.ndarray, dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate model performance using comprehensive metrics.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: True target values\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing 'predictions' and 'metrics'\n",
    "        \"\"\"\n",
    "        predictions = self.predict_distribution(X)\n",
    "        \n",
    "        # Extract target values\n",
    "        if isinstance(y, dict):\n",
    "            y_values = list(y.values())[0]\n",
    "        else:\n",
    "            y_values = y\n",
    "            \n",
    "        metrics = evaluate_predictions(y_values, predictions, self.distribution)\n",
    "        return {'predictions': predictions, 'metrics': metrics}\n",
    "\n",
    "\n",
    "class LinearRegressionGamma(BaseModel):\n",
    "    \"\"\"\n",
    "    Linear regression with configurable probability distributions for uncertainty.\n",
    "    \n",
    "    Fits a simple linear model and estimates residual variance to parameterize\n",
    "    the chosen output distribution. Uses the last feature as the primary predictor\n",
    "    (typically distance for travel time prediction).\n",
    "    \n",
    "    Model: y = Î¸ * x_last + Îµ, where Îµ follows the specified distribution\n",
    "    \n",
    "    Args:\n",
    "        distribution: Output distribution type (default: 'gamma')\n",
    "        \n",
    "    Attributes:\n",
    "        theta: Linear coefficient\n",
    "        scale: Scale parameter for distribution\n",
    "        y_mean: Target mean (for robustness)\n",
    "        y_std: Target standard deviation (for robustness)\n",
    "        \n",
    "    Example:\n",
    "        >>> model = LinearRegressionGamma(distribution='normal')\n",
    "        >>> model.fit(X_train, y_train)\n",
    "        >>> predictions = model.predict_distribution(X_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, distribution: str = 'gamma'):\n",
    "        super().__init__(distribution)\n",
    "        self.theta = None\n",
    "        self.scale = None\n",
    "        self.y_mean = None\n",
    "        self.y_std = None\n",
    "    \n",
    "    def fit(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "            y: Union[np.ndarray, dict]) -> 'LinearRegressionGamma':\n",
    "        \"\"\"\n",
    "        Fit linear regression model with robust scale estimation.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target values\n",
    "            \n",
    "        Returns:\n",
    "            self: Fitted estimator\n",
    "        \"\"\"\n",
    "        # Extract target values\n",
    "        if isinstance(y, dict):\n",
    "            y = list(y.values())[0]\n",
    "        \n",
    "        # Ensure positive values\n",
    "        y = np.maximum(y, 1e-6)\n",
    "        self.y_mean = np.mean(y)\n",
    "        self.y_std = np.std(y)\n",
    "        \n",
    "        # Use last feature as primary predictor (typically distance)\n",
    "        X = X[:, -1] if X.ndim > 1 else X\n",
    "        \n",
    "        # Fit linear coefficient using least squares\n",
    "        self.theta = abs(np.sum(X * y) / np.sum(X * X))\n",
    "        \n",
    "        # Calculate scale parameter from residuals\n",
    "        mean_pred = self.theta * X\n",
    "        residuals = y - mean_pred\n",
    "        \n",
    "        # Robust scale estimation with minimum threshold\n",
    "        raw_scale = np.var(residuals) / np.mean(mean_pred)\n",
    "        min_scale = 0.1 * self.y_std\n",
    "        self.scale = max(raw_scale, min_scale)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_distribution(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "                           n_samples: int = 4000) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate probabilistic predictions using fitted distribution.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            n_samples: Number of samples to generate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'predictions', 'mean', and 'std'\n",
    "        \"\"\"\n",
    "        # Use last feature as predictor\n",
    "        X = X[:, -1] if X.ndim > 1 else X\n",
    "        \n",
    "        # Calculate mean predictions\n",
    "        mean = np.maximum(self.theta * X, 1e-6)\n",
    "        \n",
    "        # Generate samples based on distribution type\n",
    "        samples = np.zeros((len(X), n_samples))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            if self.distribution == 'normal':\n",
    "                # Normal distribution: N(Î¼, ÏƒÂ²)\n",
    "                std = np.sqrt(self.scale)\n",
    "                samples[i] = np.maximum(\n",
    "                    np.random.normal(mean[i], std, n_samples), 1e-6\n",
    "                )\n",
    "            elif self.distribution == 'lognormal':\n",
    "                # Log-Normal distribution: LogN(Î¼, ÏƒÂ²)\n",
    "                log_mean = np.log(mean[i])\n",
    "                log_std = np.sqrt(self.scale / mean[i])\n",
    "                samples[i] = np.random.lognormal(log_mean, log_std, n_samples)\n",
    "            else:  # gamma\n",
    "                # Gamma distribution: Gamma(Î±, Î²)\n",
    "                shape = max(mean[i] / (self.scale + 1e-6), 1.0)\n",
    "                samples[i] = np.random.gamma(shape, self.scale, n_samples)\n",
    "        \n",
    "        return {\n",
    "            'predictions': samples,\n",
    "            'mean': mean,\n",
    "            'std': np.std(samples, axis=1)\n",
    "        }\n",
    "\n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:\n",
    "        \"\"\"Make point predictions.\"\"\"\n",
    "        return self.predict_distribution(X)['mean']\n",
    "\n",
    "\n",
    "class RandomForestRegressorGamma(BaseModel):\n",
    "    \"\"\"\n",
    "    Random Forest with configurable probability distributions for uncertainty.\n",
    "    \n",
    "    Uses ensemble variance from individual trees to estimate prediction uncertainty,\n",
    "    then samples from the specified distribution using tree-based statistics.\n",
    "    \n",
    "    The uncertainty estimation leverages the natural ensemble properties of \n",
    "    Random Forest to provide meaningful prediction intervals.\n",
    "    \n",
    "    Args:\n",
    "        n_estimators: Number of trees in the forest (default: 100)\n",
    "        distribution: Output distribution type (default: 'gamma')\n",
    "        \n",
    "    Attributes:\n",
    "        model: Underlying RandomForestRegressor from scikit-learn\n",
    "        \n",
    "    Example:\n",
    "        >>> rf = RandomForestRegressorGamma(n_estimators=200, distribution='lognormal')\n",
    "        >>> rf.fit(X_train, y_train)\n",
    "        >>> predictions = rf.predict_distribution(X_test)\n",
    "        >>> print(f\"Uncertainty: {predictions['std']}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators: int = 100, distribution: str = 'gamma'):\n",
    "        super().__init__(distribution)\n",
    "        self.model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            min_samples_leaf=5,      # Prevent overfitting\n",
    "            max_features='sqrt',     # Feature subsampling\n",
    "            bootstrap=True,          # Bootstrap sampling\n",
    "            random_state=42,         # Reproducibility\n",
    "            n_jobs=-1               # Use all available cores\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "            y: Union[np.ndarray, dict]) -> 'RandomForestRegressorGamma':\n",
    "        \"\"\"\n",
    "        Fit Random Forest model.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target values\n",
    "            \n",
    "        Returns:\n",
    "            self: Fitted estimator\n",
    "        \"\"\"\n",
    "        # Extract target values\n",
    "        if isinstance(y, dict):\n",
    "            y = list(y.values())[0]\n",
    "        \n",
    "        # Ensure positive values\n",
    "        y = np.maximum(y, 1e-6)\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict_distribution(self, X: Union[np.ndarray, pd.DataFrame], \n",
    "                           n_samples: int = 4000) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate probabilistic predictions using tree ensemble variance.\n",
    "        \n",
    "        Uses predictions from individual trees to estimate uncertainty,\n",
    "        then generates samples from the specified distribution.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            n_samples: Number of samples to generate per prediction\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with 'predictions', 'mean', and 'std'\n",
    "        \"\"\"\n",
    "        # Get predictions from all individual trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.model.estimators_])\n",
    "        \n",
    "        # Calculate ensemble statistics\n",
    "        mean = np.mean(tree_predictions, axis=0)\n",
    "        std = np.std(tree_predictions, axis=0)\n",
    "        \n",
    "        # Ensure non-zero standard deviation (minimum 10% of mean)\n",
    "        std = np.maximum(std, 0.1 * mean)\n",
    "        \n",
    "        # Generate samples based on distribution type\n",
    "        samples = np.zeros((len(X), n_samples))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            if self.distribution == 'normal':\n",
    "                # Normal distribution: N(Î¼, ÏƒÂ²)\n",
    "                samples[i] = np.maximum(\n",
    "                    np.random.normal(mean[i], std[i], n_samples), 1e-6\n",
    "                )\n",
    "            elif self.distribution == 'lognormal':\n",
    "                # Log-Normal distribution: LogN(Î¼, ÏƒÂ²)\n",
    "                log_mean = np.log(mean[i])\n",
    "                log_std = std[i] / mean[i]  # Coefficient of variation approximation\n",
    "                samples[i] = np.random.lognormal(log_mean, log_std, n_samples)\n",
    "            else:  # gamma\n",
    "                # Gamma distribution: Gamma(Î±, Î²)\n",
    "                # Method of moments parameter estimation\n",
    "                alpha = max((mean[i] / std[i]) ** 2, 1.0)\n",
    "                beta = max(mean[i] / (std[i] ** 2), 1e-6)\n",
    "                samples[i] = np.random.gamma(alpha, 1/beta, n_samples)\n",
    "        \n",
    "        return {\n",
    "            'predictions': samples,\n",
    "            'mean': mean,\n",
    "            'std': std\n",
    "        }\n",
    "\n",
    "    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:\n",
    "        \"\"\"Make point predictions.\"\"\"\n",
    "        return self.predict_distribution(X)['mean']\n",
    "\n",
    "\n",
    "def train_and_evaluate(model: BaseEstimator, \n",
    "                      train_df: pd.DataFrame, \n",
    "                      test_df: pd.DataFrame, \n",
    "                      input_features: List[str], \n",
    "                      target_features: List[str],\n",
    "                      model_name: str = \"Model\") -> Tuple[BaseEstimator, Optional[Dict]]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a model with comprehensive performance metrics.\n",
    "    \n",
    "    Handles the complete training and evaluation pipeline including data\n",
    "    preparation, model fitting, prediction generation, and metric calculation.\n",
    "    Provides robust error handling and informative progress reporting.\n",
    "    \n",
    "    Args:\n",
    "        model: Model instance to train (must implement fit and evaluate methods)\n",
    "        train_df: Training dataset as pandas DataFrame\n",
    "        test_df: Testing dataset as pandas DataFrame\n",
    "        input_features: List of input feature column names\n",
    "        target_features: List of target feature column names\n",
    "        model_name: Display name for progress reporting (default: \"Model\")\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trained_model, evaluation_results):\n",
    "        - trained_model: The fitted model instance\n",
    "        - evaluation_results: Dict with 'metrics' and 'predictions', or None if failed\n",
    "        \n",
    "    Example:\n",
    "        >>> model = BNNPredictor(input_dim=3, input_features=['a', 'b', 'c'], \n",
    "        ...                     target_features=['y'])\n",
    "        >>> trained_model, results = train_and_evaluate(\n",
    "        ...     model, train_df, test_df, ['a', 'b', 'c'], ['y'], \"BNN\"\n",
    "        ... )\n",
    "        >>> if results:\n",
    "        ...     print(f\"MAE: {results['metrics']['mae']:.3f}\")\n",
    "    \"\"\"\n",
    "    # Prepare training and testing data\n",
    "    X_train, y_train = prepare_data(train_df, input_features, target_features)\n",
    "    X_test, y_test = prepare_data(test_df, input_features, target_features)\n",
    "\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Test samples: {len(X_test)}\")\n",
    "    print(f\"  Features: {len(input_features)}\")\n",
    "    \n",
    "    # Training phase with error handling\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"  âœ“ Training completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Training failed: {str(e)}\")\n",
    "        return model, None\n",
    "\n",
    "    # Evaluation phase with error handling\n",
    "    try:\n",
    "        evaluation_results = model.evaluate(X_test, y_test)\n",
    "        metrics = evaluation_results['metrics']\n",
    "\n",
    "        # Display results\n",
    "        print(f\"\\n{model_name} Performance:\")\n",
    "        print(f\"  MAE:  {metrics['mae']:.3f}\")\n",
    "        print(f\"  RMSE: {metrics['rmse']:.3f}\")\n",
    "        print(f\"  NLL:  {metrics['nll']:.4f}\")\n",
    "        \n",
    "        return model, evaluation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Evaluation failed: {str(e)}\")\n",
    "        return model, None\n",
    "\n",
    "\n",
    "def run_all_models(train_df: pd.DataFrame, \n",
    "                  test_df: pd.DataFrame,\n",
    "                  input_features: List[str],\n",
    "                  target_features: List[str], \n",
    "                  distributions: Optional[Dict[str, str]] = None,\n",
    "                  bnn_prior: str = 'normal', \n",
    "                  bnn_likelihood: str = 'gamma') -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Train and evaluate all available models with configurable distributions.\n",
    "    \n",
    "    Provides a comprehensive comparison of Linear Regression, Random Forest,\n",
    "    and Bayesian Neural Network models with different distribution configurations.\n",
    "    Includes robust error handling to continue evaluation even if individual\n",
    "    models fail.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataset as pandas DataFrame\n",
    "        test_df: Testing dataset as pandas DataFrame\n",
    "        input_features: List of input feature column names\n",
    "        target_features: List of target feature column names\n",
    "        distributions: Distribution config for each model type. If None, defaults\n",
    "                      to {'linear': 'gamma', 'rf': 'gamma', 'bnn': 'gamma'}\n",
    "        bnn_prior: Prior distribution for BNN weights (default: 'normal')\n",
    "        bnn_likelihood: Likelihood distribution for BNN outputs (default: 'gamma')\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trained_models_dict, results_dict):\n",
    "        - trained_models_dict: Successfully trained model instances\n",
    "        - results_dict: Evaluation results for each successful model\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If invalid distribution types are specified\n",
    "        \n",
    "    Example:\n",
    "        >>> # Standard comparison with default gamma distributions\n",
    "        >>> models, results = run_all_models(train_df, test_df, \n",
    "        ...                                 ['dist', 'time'], ['travel_time'])\n",
    "        >>> \n",
    "        >>> # Custom distribution configuration\n",
    "        >>> custom_dists = {'linear': 'normal', 'rf': 'lognormal', 'bnn': 'gamma'}\n",
    "        >>> models, results = run_all_models(train_df, test_df, \n",
    "        ...                                 ['dist', 'time'], ['travel_time'],\n",
    "        ...                                 distributions=custom_dists,\n",
    "        ...                                 bnn_prior='gamma', bnn_likelihood='normal')\n",
    "    \"\"\"\n",
    "    # Set default distributions if not provided\n",
    "    if distributions is None:\n",
    "        distributions = {'linear': 'gamma', 'rf': 'gamma', 'bnn': 'gamma'}\n",
    "    \n",
    "    # Validate all distribution parameters\n",
    "    for model_type, dist in distributions.items():\n",
    "        if dist not in SUPPORTED_DISTRIBUTIONS:\n",
    "            raise ValueError(f\"Invalid distribution '{dist}' for {model_type}. \"\n",
    "                           f\"Choose from: {SUPPORTED_DISTRIBUTIONS}\")\n",
    "    \n",
    "    if bnn_prior not in SUPPORTED_DISTRIBUTIONS:\n",
    "        raise ValueError(f\"Invalid BNN prior '{bnn_prior}'. \"\n",
    "                        f\"Choose from: {SUPPORTED_DISTRIBUTIONS}\")\n",
    "    if bnn_likelihood not in SUPPORTED_DISTRIBUTIONS:\n",
    "        raise ValueError(f\"Invalid BNN likelihood '{bnn_likelihood}'. \"\n",
    "                        f\"Choose from: {SUPPORTED_DISTRIBUTIONS}\")\n",
    "    \n",
    "    # Initialize all models with specified configurations\n",
    "    models_config = {\n",
    "        'Linear Regression': LinearRegressionGamma(\n",
    "            distribution=distributions.get('linear', 'gamma')\n",
    "        ),\n",
    "        'Random Forest': RandomForestRegressorGamma(\n",
    "            n_estimators=100, \n",
    "            distribution=distributions.get('rf', 'gamma')\n",
    "        ),\n",
    "        'Bayesian Neural Network': BNNPredictor(\n",
    "            input_dim=len(input_features),\n",
    "            input_features=input_features,\n",
    "            target_features=target_features,\n",
    "            hidden_dim=32,\n",
    "            prior_dist=bnn_prior,\n",
    "            likelihood_dist=bnn_likelihood\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Storage for successful results\n",
    "    trained_models = {}\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # Header for model comparison\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Dataset: {len(train_df)} train, {len(test_df)} test samples\")\n",
    "    print(f\"Features: {input_features}\")\n",
    "    print(f\"Target: {target_features}\")\n",
    "    print(f\"Distributions: {distributions}\")\n",
    "    if 'Bayesian Neural Network' in models_config:\n",
    "        print(f\"BNN Config: Prior={bnn_prior}, Likelihood={bnn_likelihood}\")\n",
    "\n",
    "    # Process each model with individual error handling\n",
    "    for model_name, model_instance in models_config.items():\n",
    "        print(f\"\\n{'-'*40}\")\n",
    "        print(f\"Processing {model_name}...\")\n",
    "        \n",
    "        # Display model-specific configuration\n",
    "        if model_name == 'Bayesian Neural Network':\n",
    "            print(f\"  Prior: {bnn_prior}\")\n",
    "            print(f\"  Likelihood: {bnn_likelihood}\")\n",
    "            print(f\"  Hidden units: {model_instance.hidden_dim}\")\n",
    "        else:\n",
    "            dist_key = 'linear' if 'Linear' in model_name else 'rf'\n",
    "            print(f\"  Distribution: {distributions.get(dist_key, 'gamma')}\")\n",
    "\n",
    "        try:\n",
    "            # Train and evaluate model\n",
    "            trained_model, results = train_and_evaluate(\n",
    "                model_instance, train_df, test_df, \n",
    "                input_features, target_features, model_name\n",
    "            )\n",
    "\n",
    "            # Store successful results\n",
    "            if results is not None:\n",
    "                trained_models[model_name] = trained_model\n",
    "                evaluation_results[model_name] = results\n",
    "                print(f\"  âœ“ {model_name} completed successfully\")\n",
    "            else:\n",
    "                print(f\"  âœ— {model_name} failed during evaluation\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— {model_name} failed with error: {str(e)}\")\n",
    "            print(f\"    Continuing with remaining models...\")\n",
    "            continue\n",
    "\n",
    "    # Generate comparison summary\n",
    "    if evaluation_results:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Create formatted results table\n",
    "        print(f\"{'Model':<25} {'MAE':<8} {'RMSE':<8} {'NLL':<10} {'Distribution'}\")\n",
    "        print(f\"{'-'*65}\")\n",
    "\n",
    "        # Sort by NLL (lower is better) for ranking\n",
    "        sorted_results = sorted(evaluation_results.items(), \n",
    "                              key=lambda x: x[1]['metrics']['nll'])\n",
    "\n",
    "        for model_name, result in sorted_results:\n",
    "            metrics = result['metrics']\n",
    "            \n",
    "            # Get distribution info\n",
    "            if 'Linear' in model_name:\n",
    "                dist_info = distributions.get('linear', 'gamma')\n",
    "            elif 'Forest' in model_name:\n",
    "                dist_info = distributions.get('rf', 'gamma')\n",
    "            else:  # BNN\n",
    "                dist_info = bnn_likelihood\n",
    "            \n",
    "            print(f\"{model_name:<25} {metrics['mae']:<8.3f} {metrics['rmse']:<8.3f} \"\n",
    "                  f\"{metrics['nll']:<10.4f} {dist_info}\")\n",
    "\n",
    "        # Highlight best performing model\n",
    "        best_model_name = sorted_results[0][0]\n",
    "        best_nll = sorted_results[0][1]['metrics']['nll']\n",
    "        print(f\"\\nðŸ† Best Model: {best_model_name} (NLL: {best_nll:.4f})\")\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  No models completed successfully!\")\n",
    "\n",
    "    return trained_models, evaluation_results\n",
    "\n",
    "\n",
    "def compare_distributions(train_df: pd.DataFrame, \n",
    "                         test_df: pd.DataFrame,\n",
    "                         input_features: List[str],\n",
    "                         target_features: List[str], \n",
    "                         model_type: str = 'linear') -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Compare all supported distributions for a single model type.\n",
    "    \n",
    "    Systematically evaluates Normal, Log-Normal, and Gamma distributions\n",
    "    for the specified model type to determine optimal distribution choice.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training dataset\n",
    "        test_df: Testing dataset  \n",
    "        input_features: List of input feature column names\n",
    "        target_features: List of target feature column names\n",
    "        model_type: Model type to test ('linear', 'rf', or 'bnn')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping distribution names to their evaluation metrics\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If unknown model_type is specified\n",
    "        \n",
    "    Example:\n",
    "        >>> # Compare distributions for Random Forest\n",
    "        >>> rf_results = compare_distributions(train_df, test_df, \n",
    "        ...                                   ['distance', 'time'], ['travel_time'], \n",
    "        ...                                   model_type='rf')\n",
    "        >>> # Find best distribution\n",
    "        >>> best_dist = min(rf_results.items(), key=lambda x: x[1]['nll'])\n",
    "        >>> print(f\"Best distribution for RF: {best_dist[0]}\")\n",
    "    \"\"\"\n",
    "    if model_type not in ['linear', 'rf', 'bnn']:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. \"\n",
    "                        f\"Choose from: 'linear', 'rf', 'bnn'\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DISTRIBUTION COMPARISON FOR {model_type.upper()} MODEL\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Testing all distributions: {SUPPORTED_DISTRIBUTIONS}\")\n",
    "    \n",
    "    # Test each supported distribution\n",
    "    for dist in SUPPORTED_DISTRIBUTIONS:\n",
    "        print(f\"\\nðŸ“Š Testing {dist.upper()} distribution...\")\n",
    "        \n",
    "        try:\n",
    "            # Create model instance based on type and distribution\n",
    "            if model_type == 'linear':\n",
    "                model = LinearRegressionGamma(distribution=dist)\n",
    "                display_name = f\"Linear-{dist.capitalize()}\"\n",
    "            elif model_type == 'rf':\n",
    "                model = RandomForestRegressorGamma(n_estimators=100, distribution=dist)\n",
    "                display_name = f\"RandomForest-{dist.capitalize()}\"\n",
    "            elif model_type == 'bnn':\n",
    "                model = BNNPredictor(\n",
    "                    input_dim=len(input_features),\n",
    "                    input_features=input_features, \n",
    "                    target_features=target_features,\n",
    "                    hidden_dim=32,\n",
    "                    prior_dist=dist,\n",
    "                    likelihood_dist=dist\n",
    "                )\n",
    "                display_name = f\"BNN-{dist.capitalize()}\"\n",
    "            \n",
    "            # Train and evaluate\n",
    "            trained_model, evaluation = train_and_evaluate(\n",
    "                model, train_df, test_df, input_features, target_features, display_name\n",
    "            )\n",
    "            \n",
    "            if evaluation is not None:\n",
    "                results[dist] = evaluation['metrics']\n",
    "                print(f\"  âœ“ {dist.capitalize()} completed successfully\")\n",
    "            else:\n",
    "                print(f\"  âœ— {dist.capitalize()} evaluation failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— {dist.capitalize()} failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate comparison summary\n",
    "    if results:\n",
    "        print(f\"\\n{model_type.upper()} DISTRIBUTION COMPARISON RESULTS:\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(f\"{'Distribution':<12} {'MAE':<8} {'RMSE':<8} {'NLL':<10}\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        # Sort by NLL for ranking\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1]['nll'])\n",
    "        \n",
    "        for dist, metrics in sorted_results:\n",
    "            print(f\"{dist:<12} {metrics['mae']:<8.3f} {metrics['rmse']:<8.3f} \"\n",
    "                  f\"{metrics['nll']:<10.4f}\")\n",
    "            \n",
    "        # Highlight best distribution\n",
    "        best_dist = sorted_results[0][0]\n",
    "        best_nll = sorted_results[0][1]['nll']\n",
    "        print(f\"\\nðŸ† Best distribution for {model_type}: {best_dist} (NLL: {best_nll:.4f})\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  No distributions completed successfully for {model_type}!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_example_data(n_samples: int = 1000, \n",
    "                       random_seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic travel time dataset for testing and demonstration.\n",
    "    \n",
    "    Creates a dataset with realistic relationships between travel features and\n",
    "    travel time, including rush hour effects, traffic impacts, and weather conditions.\n",
    "    \n",
    "    Features generated:\n",
    "    - distance: Trip distance in kilometers (1-50 km)\n",
    "    - time_of_day: Hour of day (0-24)\n",
    "    - traffic_density: Traffic density factor (0.1-1.0)\n",
    "    - weather_impact: Weather condition multiplier (0.8-1.2)\n",
    "    \n",
    "    Target variable:\n",
    "    - travel_time: Realistic travel time with feature dependencies\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Total number of samples to generate (default: 1000)\n",
    "        random_seed: Random seed for reproducibility (default: 42)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_df, test_df) with 80/20 split\n",
    "        \n",
    "    Example:\n",
    "        >>> train_df, test_df = create_example_data(n_samples=500)\n",
    "        >>> print(f\"Training samples: {len(train_df)}\")\n",
    "        >>> print(f\"Features: {list(train_df.columns)}\")\n",
    "        >>> # Use for model testing\n",
    "        >>> models, results = run_all_models(train_df, test_df, \n",
    "        ...                                 ['distance', 'time_of_day'], \n",
    "        ...                                 ['travel_time'])\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    print(f\"Generating synthetic travel time dataset...\")\n",
    "    print(f\"  Samples: {n_samples}\")\n",
    "    print(f\"  Random seed: {random_seed}\")\n",
    "    \n",
    "    # Generate realistic travel features\n",
    "    distance = np.random.uniform(1, 50, n_samples)  # Trip distance (km)\n",
    "    time_of_day = np.random.uniform(0, 24, n_samples)  # Hour of day\n",
    "    traffic_density = np.random.uniform(0.1, 1.0, n_samples)  # Traffic factor\n",
    "    weather_impact = np.random.uniform(0.8, 1.2, n_samples)  # Weather factor\n",
    "    \n",
    "    # Create realistic travel time with dependencies\n",
    "    # Base time proportional to distance\n",
    "    base_time = distance * 2 + np.random.normal(0, 2, n_samples)\n",
    "    \n",
    "    # Rush hour effects (7-9 AM and 5-7 PM)\n",
    "    rush_hour_multiplier = np.where(\n",
    "        ((time_of_day >= 7) & (time_of_day <= 9)) | \n",
    "        ((time_of_day >= 17) & (time_of_day <= 19)), \n",
    "        1.3,  # 30% increase during rush hour\n",
    "        1.0\n",
    "    )\n",
    "    \n",
    "    # Traffic density effect\n",
    "    traffic_multiplier = 1 + traffic_density * 0.5  # Up to 50% increase\n",
    "    \n",
    "    # Calculate final travel time\n",
    "    travel_time = (base_time * rush_hour_multiplier * \n",
    "                  traffic_multiplier * weather_impact)\n",
    "    \n",
    "    # Ensure all travel times are positive\n",
    "    travel_time = np.maximum(travel_time, 1.0)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'distance': distance,\n",
    "        'time_of_day': time_of_day,\n",
    "        'traffic_density': traffic_density,\n",
    "        'weather_impact': weather_impact,\n",
    "        'travel_time': travel_time\n",
    "    })\n",
    "    \n",
    "    # Split into training and testing sets (80/20)\n",
    "    split_index = int(0.8 * n_samples)\n",
    "    train_df = data.iloc[:split_index].reset_index(drop=True)\n",
    "    test_df = data.iloc[split_index:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  âœ“ Generated {len(train_df)} training and {len(test_df)} test samples\")\n",
    "    print(f\"  âœ“ Travel time range: {travel_time.min():.1f} - {travel_time.max():.1f} minutes\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def main_example():\n",
    "    \"\"\"\n",
    "    Comprehensive example demonstrating the library's capabilities.\n",
    "    \n",
    "    Generates synthetic data and runs a complete analysis including:\n",
    "    - All model types with default configurations\n",
    "    - Distribution comparisons\n",
    "    - Best model selection and evaluation\n",
    "    \n",
    "    This serves as both a usage example and a validation test.\n",
    "    \"\"\"\n",
    "    print(\"ðŸš— Bayesian Travel Time Prediction Library - Comprehensive Example\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Generate example dataset\n",
    "    print(\"\\nðŸ“Š STEP 1: Read Data\")\n",
    "    trips_df = pd.read_csv(trips_input_filename)\n",
    "    train_df, test_df = train_test_split(trips_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"  Input features: {input_features}\")\n",
    "    print(f\"  Target feature: {target_features[0]}\")\n",
    "    print(f\"  Training samples: {len(train_df)}\")\n",
    "    print(f\"  Test samples: {len(test_df)}\")\n",
    "    \n",
    "    # Run comprehensive model comparison\n",
    "    print(f\"\\nðŸ¤– STEP 2: Model Comparison\")\n",
    "    models, results = run_all_models(\n",
    "        train_df, test_df, input_features, target_features,\n",
    "        distributions={'linear': 'gamma', 'rf': 'gamma', 'bnn': 'gamma'}\n",
    "    )\n",
    "    \n",
    "   \n",
    "        \n",
    "    print(f\"\\nâœ… completed successfully!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models, results =main_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cfb16ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/42, rows: 0 to 999\n",
      "Processing chunk 2/42, rows: 1000 to 1999\n",
      "Processing chunk 3/42, rows: 2000 to 2999\n",
      "Processing chunk 4/42, rows: 3000 to 3999\n",
      "Processing chunk 5/42, rows: 4000 to 4999\n",
      "Processing chunk 6/42, rows: 5000 to 5999\n",
      "Processing chunk 7/42, rows: 6000 to 6999\n",
      "Processing chunk 8/42, rows: 7000 to 7999\n",
      "Processing chunk 9/42, rows: 8000 to 8999\n",
      "Processing chunk 10/42, rows: 9000 to 9999\n",
      "Processing chunk 11/42, rows: 10000 to 10999\n",
      "Processing chunk 12/42, rows: 11000 to 11999\n",
      "Processing chunk 13/42, rows: 12000 to 12999\n",
      "Processing chunk 14/42, rows: 13000 to 13999\n",
      "Processing chunk 15/42, rows: 14000 to 14999\n",
      "Processing chunk 16/42, rows: 15000 to 15999\n",
      "Processing chunk 17/42, rows: 16000 to 16999\n",
      "Processing chunk 18/42, rows: 17000 to 17999\n",
      "Processing chunk 19/42, rows: 18000 to 18999\n",
      "Processing chunk 20/42, rows: 19000 to 19999\n",
      "Processing chunk 21/42, rows: 20000 to 20999\n",
      "Processing chunk 22/42, rows: 21000 to 21999\n",
      "Processing chunk 23/42, rows: 22000 to 22999\n",
      "Processing chunk 24/42, rows: 23000 to 23999\n",
      "Processing chunk 25/42, rows: 24000 to 24999\n",
      "Processing chunk 26/42, rows: 25000 to 25999\n",
      "Processing chunk 27/42, rows: 26000 to 26999\n",
      "Processing chunk 28/42, rows: 27000 to 27999\n",
      "Processing chunk 29/42, rows: 28000 to 28999\n",
      "Processing chunk 30/42, rows: 29000 to 29999\n",
      "Processing chunk 31/42, rows: 30000 to 30999\n",
      "Processing chunk 32/42, rows: 31000 to 31999\n",
      "Processing chunk 33/42, rows: 32000 to 32999\n",
      "Processing chunk 34/42, rows: 33000 to 33999\n",
      "Processing chunk 35/42, rows: 34000 to 34999\n",
      "Processing chunk 36/42, rows: 35000 to 35999\n",
      "Processing chunk 37/42, rows: 36000 to 36999\n",
      "Processing chunk 38/42, rows: 37000 to 37999\n",
      "Processing chunk 39/42, rows: 38000 to 38999\n",
      "Processing chunk 40/42, rows: 39000 to 39999\n",
      "Processing chunk 41/42, rows: 40000 to 40999\n",
      "Processing chunk 42/42, rows: 41000 to 41229\n",
      "Processing complete. Results saved to study areas/orlÃ©ans/virtual_trips_with_predicted_thresholds.CSV\n"
     ]
    }
   ],
   "source": [
    "def compute_normal_percentiles(mean, std, percentiles):\n",
    "    \"\"\"\n",
    "    Compute percentiles directly from normal distribution parameters.\n",
    "    \n",
    "    Args:\n",
    "        mean: Mean parameter(s) of normal distribution\n",
    "        std: Standard deviation parameter(s) of normal distribution  \n",
    "        percentiles: List of percentiles to compute (0-100)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with percentile values\n",
    "    \"\"\"\n",
    "    percentile_dict = {}\n",
    "    \n",
    "    for p in percentiles:\n",
    "        if np.isscalar(mean):\n",
    "            # Single prediction\n",
    "            percentile_val = stats.norm.ppf(p/100, loc=mean, scale=std)\n",
    "        else:\n",
    "            # Multiple predictions\n",
    "            percentile_val = np.array([\n",
    "                stats.norm.ppf(p/100, loc=mean[i], scale=std[i]) \n",
    "                for i in range(len(mean))\n",
    "            ])\n",
    "        percentile_dict[f'p{p}'] = percentile_val\n",
    "    \n",
    "    return percentile_dict\n",
    "\n",
    "def compute_lognormal_percentiles(log_mean, log_std, percentiles):\n",
    "    \"\"\"\n",
    "    Compute percentiles directly from log-normal distribution parameters.\n",
    "    \n",
    "    Args:\n",
    "        log_mean: Log-scale mean parameter(s) of log-normal distribution\n",
    "        log_std: Log-scale standard deviation parameter(s) of log-normal distribution  \n",
    "        percentiles: List of percentiles to compute (0-100)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with percentile values\n",
    "    \"\"\"\n",
    "    percentile_dict = {}\n",
    "    \n",
    "    for p in percentiles:\n",
    "        if np.isscalar(log_mean):\n",
    "            # Single prediction\n",
    "            percentile_val = stats.lognorm.ppf(p/100, s=log_std, scale=np.exp(log_mean))\n",
    "        else:\n",
    "            # Multiple predictions\n",
    "            percentile_val = np.array([\n",
    "                stats.lognorm.ppf(p/100, s=log_std[i], scale=np.exp(log_mean[i])) \n",
    "                for i in range(len(log_mean))\n",
    "            ])\n",
    "        percentile_dict[f'p{p}'] = percentile_val\n",
    "    \n",
    "    return percentile_dict\n",
    "\n",
    "def compute_gamma_percentiles(alpha, beta, percentiles):\n",
    "    \"\"\"\n",
    "    Compute percentiles directly from gamma distribution parameters.\n",
    "    \n",
    "    Args:\n",
    "        alpha: Shape parameter(s) of gamma distribution\n",
    "        beta: Rate parameter(s) of gamma distribution  \n",
    "        percentiles: List of percentiles to compute (0-100)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with percentile values\n",
    "    \"\"\"\n",
    "    percentile_dict = {}\n",
    "    \n",
    "    for p in percentiles:\n",
    "        if np.isscalar(alpha):\n",
    "            # Single prediction\n",
    "            percentile_val = stats.gamma.ppf(p/100, a=alpha, scale=1/beta)\n",
    "        else:\n",
    "            # Multiple predictions\n",
    "            percentile_val = np.array([\n",
    "                stats.gamma.ppf(p/100, a=alpha[i], scale=1/beta[i]) \n",
    "                for i in range(len(alpha))\n",
    "            ])\n",
    "        percentile_dict[f'p{p}'] = percentile_val\n",
    "    \n",
    "    return percentile_dict\n",
    "\n",
    "def predict_thresholds_distribution_based(new_data_np, model, confidence_levels=[0.95, 0.50]):\n",
    "    \"\"\"\n",
    "    Predict travel time thresholds for given confidence levels using direct distribution calculations.\n",
    "    \n",
    "    Args:\n",
    "        new_data_np: Numpy array of features\n",
    "        model: Trained model object\n",
    "        confidence_levels: List of confidence levels (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with threshold values for each confidence level\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        predictions = model.predict_distribution(new_data_np, n_samples=1000)  # We only need mean/std, not samples\n",
    "    \n",
    "    mean_pred = predictions['mean']\n",
    "    std_pred = predictions['std']\n",
    "    \n",
    "    # Convert confidence levels to percentiles\n",
    "    percentiles = [100 * (1 + conf) / 2 for conf in confidence_levels]\n",
    "    \n",
    "    thresholds = {}\n",
    "    \n",
    "    # Determine which distribution to use based on model type\n",
    "    if hasattr(model, 'distribution'):\n",
    "        distribution = model.distribution\n",
    "    elif hasattr(model, 'likelihood_dist'):\n",
    "        distribution = model.likelihood_dist\n",
    "    else:\n",
    "        # Default fallback\n",
    "        distribution = 'gamma'\n",
    "    \n",
    "    # Compute percentiles based on distribution type\n",
    "    if distribution == 'normal':\n",
    "        percentile_results = compute_normal_percentiles(mean_pred, std_pred, percentiles)\n",
    "    elif distribution == 'lognormal':\n",
    "        # For log-normal, we need to convert back to log-scale parameters\n",
    "        # Assuming mean_pred and std_pred are in original scale\n",
    "        log_mean = np.log(mean_pred)\n",
    "        log_std = std_pred / mean_pred  # Coefficient of variation approximation\n",
    "        percentile_results = compute_lognormal_percentiles(log_mean, log_std, percentiles)\n",
    "    elif distribution == 'gamma':\n",
    "        # Convert mean and std to gamma parameters (method of moments)\n",
    "        alpha = np.maximum((mean_pred / std_pred) ** 2, 1e-6)\n",
    "        beta = np.maximum(mean_pred / (std_pred ** 2), 1e-6)\n",
    "        percentile_results = compute_gamma_percentiles(alpha, beta, percentiles)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}\")\n",
    "    \n",
    "    # Map percentiles back to confidence levels\n",
    "    for i, conf in enumerate(confidence_levels):\n",
    "        percentile_key = f'p{percentiles[i]}'\n",
    "        thresholds[conf] = {\n",
    "            'threshold': percentile_results[percentile_key],\n",
    "            'mean': mean_pred,\n",
    "            'std': std_pred\n",
    "        }\n",
    "    \n",
    "    return thresholds\n",
    "\n",
    "def predict_thresholds_and_save_distribution_based(input_df, output_csv_path, model, chunk_size=1000, confidence_levels=[0.95, 0.75, 0.50]):\n",
    "    \"\"\"\n",
    "    Predict travel time thresholds for each row in the input DataFrame using direct distribution calculations,\n",
    "    and save the results along with the original data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        input_df: Pandas DataFrame containing the input data\n",
    "        output_csv_path: Path to save the output CSV file with predictions\n",
    "        model: Trained model object\n",
    "        chunk_size: Number of rows to process in each chunk\n",
    "        confidence_levels: List of confidence levels (0-1)\n",
    "    \"\"\"\n",
    "    # Check if output directory exists, create if not\n",
    "    output_dir = os.path.dirname(output_csv_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Calculate number of chunks\n",
    "    n_chunks = (len(input_df) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "    \n",
    "    # Flag to determine if we need to write header to output file\n",
    "    first_chunk = True\n",
    "    \n",
    "    # Process each chunk\n",
    "    for i in range(n_chunks):\n",
    "        # Extract chunk\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(input_df))\n",
    "        chunk_df = input_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        print(f\"Processing chunk {i+1}/{n_chunks}, rows: {start_idx} to {end_idx-1}\")\n",
    "        \n",
    "        # Convert features to numpy array for prediction\n",
    "        # You'll need to define input_features somewhere in your code\n",
    "        # feature_cols = input_features  # Make sure this is defined\n",
    "        feature_cols = ['origin_lat', 'origin_lon', 'destination_lat', 'destination_lon',\n",
    "                       'departure_time_hour', 'departure_time_minute',\n",
    "                       'departure_time_day_of_week', 'departure_time_day_of_month',\n",
    "                       'departure_time_month', 'departure_time_hour_sin',\n",
    "                       'departure_time_hour_cos', 'departure_time_day_of_week_sin',\n",
    "                       'departure_time_day_of_week_cos', 'departure_time_month_sin',\n",
    "                       'departure_time_month_cos', 'distance']\n",
    "        \n",
    "        features_np = chunk_df[feature_cols].values\n",
    "        \n",
    "        # Get predictions for this chunk using distribution-based method\n",
    "        thresholds = predict_thresholds_distribution_based(features_np, model, confidence_levels)\n",
    "        \n",
    "        # Add prediction columns to output dataframe\n",
    "        for conf in confidence_levels:\n",
    "            # Add threshold value for this confidence level\n",
    "            chunk_df[f'threshold_{int(conf*100)}'] = thresholds[conf]['threshold']\n",
    "        \n",
    "        # Write to output file (append mode after first chunk)\n",
    "        if first_chunk:\n",
    "            chunk_df.to_csv(output_csv_path, index=False, mode='w')\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk_df.to_csv(output_csv_path, index=False, mode='a', header=False)\n",
    "    \n",
    "    print(f\"Processing complete. Results saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(generated_virtual_drt_trips)\n",
    "\n",
    "# Create a mapping dictionary for renaming\n",
    "column_mapping = {\n",
    "    'start_x': 'origin_lat',\n",
    "    'start_y': 'origin_lon', \n",
    "    'end_x': 'destination_lat',\n",
    "    'end_y': 'destination_lon',\n",
    "    'road_distance': 'distance',\n",
    "    'departure_time_hour': 'departure_time_hour',\n",
    "    'departure_time_minute': 'departure_time_minute',\n",
    "    'departure_time_day_of_week': 'departure_time_day_of_week',\n",
    "    'departure_time_day_of_month': 'departure_time_day_of_month',\n",
    "    'departure_time_month': 'departure_time_month',\n",
    "    'departure_time_hour_sin': 'departure_time_hour_sin',\n",
    "    'departure_time_hour_cos': 'departure_time_hour_cos',\n",
    "    'departure_time_day_of_week_sin': 'departure_time_day_of_week_sin',\n",
    "    'departure_time_day_of_week_cos': 'departure_time_day_of_week_cos',\n",
    "    'departure_time_month_sin': 'departure_time_month_sin',\n",
    "    'departure_time_month_cos': 'departure_time_month_cos'\n",
    "}\n",
    "\n",
    "\n",
    "df_renamed = df.rename(columns=column_mapping)\n",
    "final_columns = ['origin_lat', 'origin_lon', 'destination_lat', 'destination_lon',\n",
    "                 'departure_time_hour', 'departure_time_minute',\n",
    "                 'departure_time_day_of_week', 'departure_time_day_of_month',\n",
    "                 'departure_time_month', 'departure_time_hour_sin',\n",
    "                 'departure_time_hour_cos', 'departure_time_day_of_week_sin',\n",
    "                 'departure_time_day_of_week_cos', 'departure_time_month_sin',\n",
    "                 'departure_time_month_cos', 'distance']\n",
    "df_final = df_renamed[final_columns]\n",
    "\n",
    "\n",
    "# Call the function to predict and save\n",
    "predict_thresholds_and_save(df_final, trips_output_path_generated_drt_trips, models[\"Random Forest\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e10537",
   "metadata": {
    "id": "b5e10537"
   },
   "source": [
    "##  Create GTFS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b47fdc44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 1272,
     "status": "error",
     "timestamp": 1743436517617,
     "user": {
      "displayName": "Giovanni CalabrÃ²",
      "userId": "13146937522042262894"
     },
     "user_tz": -120
    },
    "id": "b47fdc44",
    "outputId": "2b371892-be7f-43f7-88cf-3357e949f818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GTFS files...\n",
      "Creating stops.txt...\n",
      "Creating unified stops...\n",
      "Created 11 unique stops\n",
      "Creating stop_times.txt...\n",
      "Creating stop times for 41230 trips...\n",
      "Creating trips.txt...\n",
      "Creating trips for 41230 unique trips...\n",
      "Creating routes.txt...\n",
      "Creating routes for 41230 unique routes...\n",
      "Creating calendar.txt...\n",
      "Creating agency.txt...\n",
      "Creating GTFS text files...\n",
      "- Created stops.txt with 11 rows\n",
      "- Created stop_times.txt with 82460 rows\n",
      "- Created trips.txt with 41230 rows\n",
      "- Created routes.txt with 41230 rows\n",
      "- Created calendar.txt with 1 rows\n",
      "- Created agency.txt with 1 rows\n",
      "Creating zip archive...\n",
      "GTFS feed successfully saved to: ./GTFS/orleans/DRT_SF/gtfs_DRT_95p.zip\n",
      "Cleaning up temporary files...\n"
     ]
    }
   ],
   "source": [
    "'''import sys\n",
    "sys.path.append('/content/drive/MyDrive/ColabNotebooks/AccessibilityDRTcityChrone_CL/library/')  # Path to the folder containing the .py file\n",
    "from create_GTFS_from_Simulated_Trips import *  # Import all functions from the .py file'\n",
    "'''\n",
    "from datetime import datetime, timedelta\n",
    "import zipfile\n",
    "import os\n",
    "#from tqdm import tqdm  # Using regular tqdm instead of tqdm.notebook\n",
    "\n",
    "### input variable to create GTFS ###\n",
    "percentile_travel_time = 95\n",
    "\n",
    "\n",
    "\n",
    "# *** Define configuration with your column names ***\n",
    "config = {\n",
    "    'region': 'orleans',  # replace with your region\n",
    "    'threshold_threshold': f\"threshold_{percentile_travel_time}\",  # or whichever travel time column you want to use\n",
    "    'coord_columns': {\n",
    "        'origin_lat': 'origin_lat',\n",
    "        'origin_lon': 'origin_lon',\n",
    "        'destination_lat': 'destination_lat',\n",
    "        'destination_lon': 'destination_lon'\n",
    "    },\n",
    "    'time_columns': {\n",
    "        'departure_hour': 'departure_time_hour',\n",
    "        'departure_min': 'departure_time_minute'\n",
    "    },\n",
    "    'start_date': day_gtfs,\n",
    "    'end_date': day_gtfs,\n",
    "    'service_days': {\n",
    "        'monday': 1, 'tuesday': 1, 'wednesday': 1, 'thursday': 1, 'friday': 1,\n",
    "        'saturday': 0, 'sunday': 0\n",
    "    },\n",
    "    'agency_info': {\n",
    "        'agency_id': scenario_name,\n",
    "        'agency_name': '{scenario_name} Service',\n",
    "        'agency_url': 'https://example.com',\n",
    "        'agency_timezone': 'Europe/Rome'\n",
    "    },\n",
    "    # replace with your desired output path\n",
    "    \"scenario_name\": scenario_name,  # replace with your scenario name\n",
    "    \"output_path\": directory_GTFS\n",
    "}\n",
    "# *** adjust the columns above if they have different names ***\n",
    "\n",
    "def create_calendar(start_date, end_date, service_days):\n",
    "    \"\"\"Create calendar.txt with service dates based on user parameters\"\"\"\n",
    "    calendar_data = {\n",
    "        'service_id': ['WEEKDAY'],\n",
    "        'monday': [service_days.get('monday', 0)],\n",
    "        'tuesday': [service_days.get('tuesday', 0)],\n",
    "        'wednesday': [service_days.get('wednesday', 0)],\n",
    "        'thursday': [service_days.get('thursday', 0)],\n",
    "        'friday': [service_days.get('friday', 0)],\n",
    "        'saturday': [service_days.get('saturday', 0)],\n",
    "        'sunday': [service_days.get('sunday', 0)],\n",
    "        'start_date': [start_date],\n",
    "        'end_date': [end_date]\n",
    "    }\n",
    "    return pd.DataFrame(calendar_data)\n",
    "\n",
    "def create_agency(agency_info):\n",
    "    \"\"\"Create agency.txt with user-defined parameters\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'agency_id': [agency_info.get('agency_id', scenario_name)],\n",
    "        'agency_name': [agency_info.get('agency_name', '{scenario_name} Service')],\n",
    "        'agency_url': [agency_info.get('agency_url', 'https://example.com')],\n",
    "        'agency_timezone': [agency_info.get('agency_timezone', 'Europe/Rome')]\n",
    "    })\n",
    "\n",
    "def create_stops(trips_df, region, coord_columns):\n",
    "    \"\"\"\n",
    "    Create stops.txt with unique points and return a mapping dict for coordinates to stop_ids\n",
    "    \n",
    "    Returns:\n",
    "    - stops_df: DataFrame with unique stops\n",
    "    - coords_to_stop_id: Dictionary mapping (lat, lon) tuples to stop_ids\n",
    "    \"\"\"\n",
    "    # Extract column names\n",
    "    origin_lat = coord_columns.get('origin_lat', 'origin_lat')\n",
    "    origin_lon = coord_columns.get('origin_lon', 'origin_lon')\n",
    "    destination_lat = coord_columns.get('destination_lat', 'destination_lat')\n",
    "    destination_lon = coord_columns.get('destination_lon', 'destination_lon')\n",
    "    \n",
    "    print(\"Creating unified stops...\")\n",
    "    \n",
    "    # Create a dictionary to store unique coordinates and their stop IDs\n",
    "    coords_to_stop_id = {}\n",
    "    stop_data = []\n",
    "    stop_idx = 0\n",
    "    \n",
    "    # Process all coordinates (both origins and destinations)\n",
    "    for idx, row in trips_df.iterrows():\n",
    "        # Process origin coordinates\n",
    "        origin_coords = (row[origin_lat], row[origin_lon])\n",
    "        if origin_coords not in coords_to_stop_id:\n",
    "            stop_id = f\"stop_{stop_idx}\"\n",
    "            coords_to_stop_id[origin_coords] = stop_id\n",
    "            stop_data.append({\n",
    "                'stop_id': stop_id,\n",
    "                'stop_name': f'Stop_{stop_idx}',\n",
    "                'stop_lat': origin_coords[0],\n",
    "                'stop_lon': origin_coords[1],\n",
    "                'city': region.lower(),\n",
    "                'file': region.lower(),\n",
    "                'pos': stop_idx\n",
    "            })\n",
    "            stop_idx += 1\n",
    "        \n",
    "        # Process destination coordinates\n",
    "        dest_coords = (row[destination_lat], row[destination_lon])\n",
    "        if dest_coords not in coords_to_stop_id:\n",
    "            stop_id = f\"stop_{stop_idx}\"\n",
    "            coords_to_stop_id[dest_coords] = stop_id\n",
    "            stop_data.append({\n",
    "                'stop_id': stop_id,\n",
    "                'stop_name': f'Stop_{stop_idx}',\n",
    "                'stop_lat': dest_coords[0],\n",
    "                'stop_lon': dest_coords[1],\n",
    "                'city': region.lower(),\n",
    "                'file': region.lower(),\n",
    "                'pos': stop_idx\n",
    "            })\n",
    "            stop_idx += 1\n",
    "    \n",
    "    # Create stops DataFrame\n",
    "    stops_df = pd.DataFrame(stop_data)\n",
    "    \n",
    "    print(f\"Created {len(stops_df)} unique stops\")\n",
    "    \n",
    "    # Return both the stops DataFrame and the mapping dictionary\n",
    "    return stops_df, coords_to_stop_id\n",
    "\n",
    "def create_stop_times(trips_df, travel_time_threshold, time_columns, coord_columns, coords_to_stop_id):\n",
    "    \"\"\"\n",
    "    Create stop_times.txt using a pre-existing coordinate to stop_id mapping\n",
    "    \n",
    "    Parameters:\n",
    "    - trips_df: DataFrame with trip data\n",
    "    - travel_time_threshold: Column name with travel time data\n",
    "    - time_columns: Dict mapping time column names\n",
    "    - coord_columns: Dict mapping coordinate column names\n",
    "    - coords_to_stop_id: Dictionary mapping (lat, lon) tuples to stop_ids\n",
    "    \"\"\"\n",
    "    trip_ids = []\n",
    "    arrival_times = []\n",
    "    departure_times_list = []\n",
    "    stop_ids = []\n",
    "    stop_sequences = []\n",
    "    \n",
    "    # Extract column names\n",
    "    origin_lat = coord_columns.get('origin_lat', 'origin_lat')\n",
    "    origin_lon = coord_columns.get('origin_lon', 'origin_lon')\n",
    "    destination_lat = coord_columns.get('destination_lat', 'destination_lat')\n",
    "    destination_lon = coord_columns.get('destination_lon', 'destination_lon')\n",
    "    departure_hour = time_columns.get('departure_hour', 'departure_hour')\n",
    "    departure_min = time_columns.get('departure_min', 'departure_min')\n",
    "    \n",
    "    print(f\"Creating stop times for {len(trips_df)} trips...\")\n",
    "    for idx, row in trips_df.iterrows():\n",
    "        # Format departure time\n",
    "        dep_time = f\"{int(row[departure_hour]):02d}:{int(row[departure_min]):02d}:00\"\n",
    "        base_time = datetime.strptime(dep_time, \"%H:%M:%S\")\n",
    "        \n",
    "        # Calculate arrival time\n",
    "        travel_time = timedelta(seconds=int(float(row[travel_time_threshold])))\n",
    "        arr_time = (base_time + travel_time).strftime(\"%H:%M:%S\")\n",
    "        trip_id = f'trip_{idx}'\n",
    "        \n",
    "        # Get stop IDs from the mapping\n",
    "        dep_stop_id = coords_to_stop_id[(row[origin_lat], row[origin_lon])]\n",
    "        arr_stop_id = coords_to_stop_id[(row[destination_lat], row[destination_lon])]\n",
    "        \n",
    "        # Add to lists\n",
    "        trip_ids.extend([trip_id, trip_id])\n",
    "        arrival_times.extend([dep_time, arr_time])\n",
    "        departure_times_list.extend([dep_time, arr_time])\n",
    "        stop_ids.extend([dep_stop_id, arr_stop_id])\n",
    "        stop_sequences.extend([1, 2])\n",
    "    \n",
    "    stop_times = pd.DataFrame({\n",
    "        'trip_id': trip_ids,\n",
    "        'arrival_time': arrival_times,\n",
    "        'departure_time': departure_times_list,\n",
    "        'stop_id': stop_ids,\n",
    "        'stop_sequence': stop_sequences\n",
    "    })\n",
    "    \n",
    "    return stop_times\n",
    "\n",
    "def create_trips(stop_times, region):\n",
    "    \"\"\"Create trips.txt with updated trip format\"\"\"\n",
    "    unique_trips = stop_times['trip_id'].unique()\n",
    "    print(f\"Creating trips for {len(unique_trips)} unique trips...\")\n",
    "    trips = pd.DataFrame({\n",
    "        'route_id': [f'route_{trip_id}' for trip_id in unique_trips],\n",
    "        'service_id': ['WEEKDAY' for _ in unique_trips],\n",
    "        'trip_id': unique_trips,\n",
    "        'trip_headsign': [f'DRT Service {trip_id}' for trip_id in unique_trips],\n",
    "        'city': [region.lower() for _ in unique_trips],\n",
    "        'file': [region.lower() for _ in unique_trips]\n",
    "    })\n",
    "    return trips\n",
    "\n",
    "def create_routes(trips, region):\n",
    "    \"\"\"Create routes.txt with updated format\"\"\"\n",
    "    unique_routes = trips['route_id'].unique()\n",
    "    print(f\"Creating routes for {len(unique_routes)} unique routes...\")\n",
    "    routes = pd.DataFrame({\n",
    "        'route_id': unique_routes,\n",
    "        'route_type': [3 for _ in range(len(unique_routes))],\n",
    "        'route_short_name': [f'DRT_{i}' for i in range(len(unique_routes))],\n",
    "        'agency_id': ['DRT_1' for _ in range(len(unique_routes))],\n",
    "        'route_long_name': [f'DRT Service Route {i}' for i in range(len(unique_routes))],\n",
    "        'city': [region.lower() for _ in range(len(unique_routes))],\n",
    "        'file': [region.lower() for _ in range(len(unique_routes))]\n",
    "    })\n",
    "    return routes\n",
    "\n",
    "def create_gtfs_from_trips(trips_df, config):\n",
    "    \"\"\"Convert trips dataframe to GTFS format with user-defined parameters\"\"\"\n",
    "    gtfs = {}\n",
    "    \n",
    "    region = config.get('region', 'orleans')\n",
    "    travel_time_threshold = config.get('travel_time_threshold', 'threshold_95')\n",
    "    coord_columns = config.get('coord_columns', {\n",
    "        'origin_lat': 'origin_lat',\n",
    "        'origin_lon': 'origin_lon',\n",
    "        'destination_lat': 'destination_lat',\n",
    "        'destination_lon': 'destination_lon'\n",
    "    })\n",
    "    time_columns = config.get('time_columns', {\n",
    "        'departure_hour': 'departure_hour',\n",
    "        'departure_min': 'departure_min'\n",
    "    })\n",
    "    \n",
    "    print(\"Creating GTFS files...\")\n",
    "    \n",
    "    print(\"Creating stops.txt...\")\n",
    "    stops, coords_to_stop_id = create_stops(trips_df, region, coord_columns)\n",
    "    gtfs['stops'] = stops\n",
    "    \n",
    "    print(\"Creating stop_times.txt...\")\n",
    "    stop_times = create_stop_times(trips_df, travel_time_threshold, time_columns, coord_columns, coords_to_stop_id)\n",
    "    gtfs['stop_times'] = stop_times\n",
    "    \n",
    "    print(\"Creating trips.txt...\")\n",
    "    trips = create_trips(stop_times, region)\n",
    "    gtfs['trips'] = trips\n",
    "    \n",
    "    print(\"Creating routes.txt...\")\n",
    "    routes = create_routes(trips, region)\n",
    "    gtfs['routes'] = routes\n",
    "    \n",
    "    print(\"Creating calendar.txt...\")\n",
    "    service_days = config.get('service_days', {\n",
    "        'monday': 1, 'tuesday': 1, 'wednesday': 1, 'thursday': 1, 'friday': 1,\n",
    "        'saturday': 0, 'sunday': 0\n",
    "    })\n",
    "    calendar = create_calendar(\n",
    "        config.get('start_date', '20250401'),\n",
    "        config.get('end_date', '20250402'),\n",
    "        service_days\n",
    "    )\n",
    "    gtfs['calendar'] = calendar\n",
    "    \n",
    "    print(\"Creating agency.txt...\")\n",
    "    agency_info = config.get('agency_info', {\n",
    "        'agency_id': 'DRT_1',\n",
    "        'agency_name': 'DRT Service',\n",
    "        'agency_url': 'https://example.com',\n",
    "        'agency_timezone': 'Europe/Paris'\n",
    "    })\n",
    "    agency = create_agency(agency_info)\n",
    "    gtfs['agency'] = agency\n",
    "    \n",
    "    return gtfs\n",
    "\n",
    "def save_gtfs_files(gtfs, output_path, scenario_name):\n",
    "    \"\"\"Save GTFS files into a zip archive\"\"\"\n",
    "    temp_dir = os.path.join(output_path, 'temp_gtfs')\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    zip_path = os.path.join(output_path, f'gtfs_{scenario_name}.zip')\n",
    "    \n",
    "    try:\n",
    "        print(\"Creating GTFS text files...\")\n",
    "        for filename, df in gtfs.items():\n",
    "            file_path = os.path.join(temp_dir, f\"{filename}.txt\")\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"- Created {filename}.txt with {len(df)} rows\")\n",
    "        \n",
    "        print(\"Creating zip archive...\")\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for filename in os.listdir(temp_dir):\n",
    "                file_path = os.path.join(temp_dir, filename)\n",
    "                zipf.write(file_path, filename)\n",
    "        \n",
    "        print(f\"GTFS feed successfully saved to: {zip_path}\")\n",
    "        \n",
    "    finally:\n",
    "        print(\"Cleaning up temporary files...\")\n",
    "        for filename in os.listdir(temp_dir):\n",
    "            os.remove(os.path.join(temp_dir, filename))\n",
    "        os.rmdir(temp_dir)\n",
    "\n",
    "# Example: Creating a small sample dataset for testing\n",
    "# Replace this with your actual data loading\n",
    "def create_sample_data(num_trips=10):\n",
    "    \"\"\"Create a small sample dataset for testing\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create random coordinates\n",
    "    origins = [(47.90 + np.random.random()*0.1, 1.90 + np.random.random()*0.1) for _ in range(5)]\n",
    "    destinations = [(47.95 + np.random.random()*0.1, 1.95 + np.random.random()*0.1) for _ in range(5)]\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_trips):\n",
    "        origin = origins[np.random.randint(0, len(origins))]\n",
    "        destination = destinations[np.random.randint(0, len(destinations))]\n",
    "        \n",
    "        trip = {\n",
    "            'origin_lat': origin[0],\n",
    "            'origin_lon': origin[1],\n",
    "            'destination_lat': destination[0],\n",
    "            'destination_lon': destination[1],\n",
    "            'departure_hour': np.random.randint(7, 20),\n",
    "            'departure_min': np.random.randint(0, 60),\n",
    "            'travel_time_25': np.random.randint(500, 1500),\n",
    "            'travel_time_50': np.random.randint(600, 1800),\n",
    "            'travel_time_70': np.random.randint(700, 2100),\n",
    "            'travel_time_95': np.random.randint(800, 2400)\n",
    "        }\n",
    "        data.append(trip)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Run the GTFS generation process\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# To modify the configuration and run again:\n",
    "# ------------------------------------------------------------------\n",
    "# Configuration - customize these values as needed\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your trips DataFrame\n",
    "    trips_df = pd.read_csv(trips_output_path_generated_drt_trips)\n",
    "    \n",
    "    # Create GTFS files\n",
    "    gtfs = create_gtfs_from_trips(trips_df, config)\n",
    "    \n",
    "    # Save GTFS files\n",
    "    save_gtfs_files(gtfs, directory_GTFS, f\"{scenario_name}_{percentile_travel_time}p\")\n",
    "\n",
    "# # Update configuration as needed\n",
    "# config['region'] = 'paris'\n",
    "# config['travel_timee_threshold'] = 'travel_time_50'\n",
    "# config['scenario_name'] = 'paris_scenario'\n",
    "# config['service_days']['saturday'] = 1  # Add Saturday service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb4ef6",
   "metadata": {},
   "source": [
    "### Merge GTFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f64ed",
   "metadata": {
    "id": "fc3f64ed"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Using default threshold (travel_time_25)\n",
    "gtfs_data = create_gtfs_from_trips(data,\"travel_time_25\")\n",
    "\n",
    "# Or specifying a different threshold\n",
    "gtfs_data_25 = create_gtfs_from_trips(data, travel_time_threshold='travel_time_25')\n",
    "\n",
    "# Save with default threshold\n",
    "save_gtfs_files(gtfs_data, \"path/to/output\")\n",
    "\n",
    "# Or save with specific threshold\n",
    "save_gtfs_files(gtfs_data_95, \"path/to/output\", travel_time_threshold='travel_time_25')\n",
    "\n",
    "# For joining and exporting GTFS files:\n",
    "merged_gtfs = joinGTFS(gtfs_dataset1, gtfs_dataset2)\n",
    "exportGTFS(\"path/to/output\", merged_gtfs)  # These functions don't need the threshold parameter'\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xH0z3DjlycgV",
    "FSWAHV6PyngN",
    "h6Sz8hUJzG1-",
    "5Df4vJv-yySG",
    "DoSMkvNlzXqG",
    "d7ad28c8"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1eh45II06nDxtk92XHR5jiiwfKtmzRdma",
     "timestamp": 1743085429553
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
